import csv
import json
import shlex
import traceback
from abc import ABC
from typing import Dict, Iterable, List, Tuple, Union

from ufdl.joblauncher.core import load_class
from ufdl.joblauncher.core.executors import AbstractJobExecutor, AbstractPredictJobExecutor
from ufdl.joblauncher.core.executors.descriptors import Parameter
from ufdl.jobtypes.base import String
from ufdl.jobtypes.standard.container import Array
from ufdl.json.object_detection import ImageAnnotation, Polygon, VideoAnnotation
from ufdl.pythonclient.functional.object_detection.dataset import get_metadata, set_metadata, set_annotations_for_file
from wai.annotations.roi.util import ROIObject
from wai.json.object import Absent

from .confidence import AbstractConfidenceScore


def read_rois(csv_file: str) -> Tuple[List[ImageAnnotation], List[float]]:
    """
    Loads the specified ROIs CSV file and generates a list of Annotation objects
    and a list of scores from it.

    :param csvfile: the CSV file to read
    :return: the tuple of annotations list and scores list
    """
    with open(csv_file, "r") as cf:
        reader = csv.DictReader(cf)
        roi_objects = [
            ROIObject.from_dict(row)
            for row in reader
        ]

    annotations = [
        ImageAnnotation(
            x=roi_object.x,
            y=roi_object.y,
            width=roi_object.w,
            height=roi_object.h,
            label=roi_object.label_str,
            polygon=(
                Polygon.from_geometric_polygon(roi_object.polygon()) if roi_object.has_polygon()
                else Absent
            )
        )
        for roi_object in roi_objects
    ]

    scores = [
        roi_object.score
        for roi_object in roi_objects
    ]

    return annotations, scores


def store_annotations(
        executor: AbstractJobExecutor,
        dataset_pk: int,
        img_name: str,
        annotations: Union[List[ImageAnnotation], List[VideoAnnotation]]
):
    """
    Stores the annotations in the backend.

    :param executor: the executor class this is done for
    :param dataset_pk: the PK of the dataset these scores are calculated for
    :param img_name: the name of the image the scores were calculated for
    :param annotations: the list of Annotation objects
    """
    # set annotations for image
    try:
        set_annotations_for_file(executor.context, dataset_pk, img_name, annotations)
    except:
        executor.log_msg(
            f"Failed to add annotations generated by job {executor.job_pk} to dataset {dataset_pk}!\n"
            f"{traceback.format_exc()}"
        )


def store_scores(
        executor: AbstractJobExecutor,
        dataset_pk: int,
        img_name: str,
        scores:  Union[List[float], Dict[str, List[float]]]
):
    """
    Stores the annotations in the backend.

    :param executor: the executor class this is done for
    :type executor: AbstractJobExecutor
    :param dataset_pk: the PK of the dataset these scores are calculated for
    :type dataset_pk: int
    :param img_name: the name of the image the scores were calculated for
    :type img_name: str
    :param scores: the list of float scores
    :type scores: list[float]
    """
    try:
        metadata = get_metadata(executor.context, dataset_pk, img_name)
        if metadata == "":
            metadata = dict()
        else:
            metadata = json.loads(metadata)
        metadata['scores'] = scores
        set_metadata(executor.context, dataset_pk, img_name, json.dumps(metadata))
    except:
        executor.log_msg("Failed to add scores of job %d for image %s in dataset %d!\n%s" % (executor.job_pk, img_name, dataset_pk, traceback.format_exc()))


def calculate_confidence_scores(
        executor: AbstractJobExecutor,
        dataset_pk: int,
        img_name: str,
        confidence_score_classes: Iterable[str],
        annotations: List[ImageAnnotation],
        scores: List[float]
):
    """
    Calculates and stores confidence scores.

    :param executor: the executor class this is done for
    :param dataset_pk: the PK of the dataset these scores are calculated for
    :param img_name: the name of the image the scores were calculated for
    :param confidence_score_classes: the list of class names
    :param annotations: the list of annotations to use for the score confidence calculation
    :param scores: the list of scores to use for the confidence score calculation
    """

    # instantiate calculators
    conf_score_obj: List[AbstractConfidenceScore] = []
    for c in confidence_score_classes:
        try:
            cls = load_class(c)
            if not issubclass(cls, AbstractConfidenceScore):
                executor.log_msg(f"Confidence score class '{c}' does not sub-class {AbstractConfidenceScore.__qualname__}")
                continue
            conf_score_obj.append(cls())
        except:
            executor.log_msg(
                f"Failed to instantiate confidence score class: {c}\n"
                f"{traceback.format_exc()}"
            )

    # calculate the scores
    if len(conf_score_obj) > 0:
        try:
            conf_scores = dict()
            for c in conf_score_obj:
                current = c.calculate(annotations, scores)
                for k in current:
                    conf_scores[k] = current[k]
            metadata = get_metadata(executor.context, dataset_pk, img_name)
            if metadata == "":
                metadata = dict()
            else:
                metadata = json.loads(metadata)
            metadata['confidence'] = conf_scores
            set_metadata(executor.context, dataset_pk, img_name, json.dumps(metadata))
        except:
            executor.log_msg(
                f"Failed to add confidence scores of job {executor.job_pk} "
                f"for image {img_name} in dataset {dataset_pk}!\n"
                f"{traceback.format_exc()}"
            )


class AbstractObjDetPredictJobExecutor(AbstractPredictJobExecutor, ABC):
    # Which video frames to extract for prediction
    unlabelled: Tuple[str, ...] = Parameter(
        Array(String())
    )

    def _download_dataset(
            self,
            pk: int,
            output_dir: str,
            additional_source_options: Union[str, Tuple[str, ...]] = tuple()
    ):
        options: List[str] = (
            shlex.split(additional_source_options) if isinstance(additional_source_options, str)
            else list(additional_source_options)
        )

        if len(self.unlabelled) != 0:
            options += ["--unlabelled", *self.unlabelled]

        super()._download_dataset(
            pk,
            output_dir,
            tuple(options)
        )
